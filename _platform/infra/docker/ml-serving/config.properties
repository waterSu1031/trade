# TorchServe Configuration for PatchTST Model
# https://github.com/pytorch/serve/blob/master/docs/configuration.md

# Basic settings
inference_address=http://0.0.0.0:8080
management_address=http://0.0.0.0:8081
metrics_address=http://0.0.0.0:8082
number_of_netty_threads=4
job_queue_size=100

# Model store path (mounted via volume)
model_store=/home/model-server/model-store
load_models=all

# Logging
log_location=logs
log_level=INFO

# Default workers per model
default_workers_per_model=2

# Batch processing for better throughput
batch_size=8
max_batch_delay=100

# Response timeout (5 minutes for complex predictions)
default_response_timeout=300

# Enable model metrics
enable_metrics_api=true
metrics_format=prometheus

# CORS settings for frontend access
cors_allowed_origin=*
cors_allowed_methods=GET, POST, PUT, OPTIONS
cors_allowed_headers=*

# JVM options for memory management
vmargs=-Xmx2g -Xms1g

# Install dependencies at runtime if needed
install_py_dep_per_model=true