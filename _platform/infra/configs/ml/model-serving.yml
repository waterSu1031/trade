# TorchServe Configuration for PatchTST Model
# This file configures the model serving settings for PyTorch models

# Model Configuration
models:
  patch_tst:
    # Model archive file name (without .mar extension)
    name: patch_tst_trade
    # Initial number of workers
    min_workers: 1
    max_workers: 4
    # Batch size for inference
    batch_size: 8
    # Maximum batch delay in ms
    max_batch_delay: 100
    # Response timeout in seconds
    response_timeout: 300
    # Model version
    version: "1.0"

# PatchTST Specific Configuration
patch_tst:
  # Time series configuration
  context_length: 512      # Historical context window
  prediction_length: 96    # Future prediction horizon
  patch_length: 16        # Length of each patch
  stride: 8               # Stride for patch extraction
  
  # Model architecture
  d_model: 128            # Transformer dimension
  n_heads: 8              # Number of attention heads
  n_layers: 3             # Number of transformer layers
  
  # Input configuration
  num_features: 1         # Univariate or multivariate
  scaling: true           # Enable input scaling
  
# API Configuration
api:
  # Inference endpoint
  inference:
    port: 8080
    protocol: http
  # Management endpoint
  management:
    port: 8081
    protocol: http
  # Metrics endpoint
  metrics:
    port: 8082
    protocol: http
    format: prometheus

# Performance Tuning
performance:
  # Worker configuration
  default_workers_per_model: 2
  job_queue_size: 100
  
  # Batching for throughput
  enable_batching: true
  max_batch_size: 8
  max_batch_delay_ms: 100
  
  # Memory settings
  max_request_size: 67108864  # 64MB
  max_response_size: 67108864  # 64MB

# Logging and Monitoring
monitoring:
  log_level: INFO
  log_location: /home/model-server/logs
  enable_metrics: true
  metrics_interval: 60  # seconds
  
# Model Store Configuration
model_store:
  path: /home/model-server/model-store
  polling_interval: 30  # seconds
  load_models: all      # Load all models at startup